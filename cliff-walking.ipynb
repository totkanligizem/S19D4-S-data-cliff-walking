{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# â›°ï¸ UÃ§urum YÃ¼rÃ¼yÃ¼ÅŸÃ¼\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "Bu meydan okumada, pekiÅŸtirmeli Ã¶ÄŸrenme modellerini eÄŸitmek iÃ§in Gymnasium ve Stable Baselines3 kullanÄ±mÄ±nÄ±n temel yÃ¶nlerini Ã¶ÄŸreneceksiniz. Bu meydan okuma, ortamlarÄ± kurma, modelleri eÄŸitme, performanslarÄ±nÄ± gÃ¶rselleÅŸtirme ve son olarak eÄŸitilmiÅŸ bir modeli bir ortamla etkileÅŸim kurmak iÃ§in kullanma konularÄ±nda bÃ¼tÃ¼ncÃ¼l bir anlayÄ±ÅŸ kazandÄ±rmanÄ±z iÃ§in tasarlanmÄ±ÅŸtÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "TÃ¼m import ifadelerinizi notebook'unuzun en Ã¼stÃ¼ne koyarak notebook'unuzu tekrar Ã§alÄ±ÅŸtÄ±rmayÄ± kolaylaÅŸtÄ±rmak iyi bir alÄ±ÅŸkanlÄ±ktÄ±r.\n",
    "\n",
    "Meydan okumalar boyunca import ifadeleri eklemeniz gerekecek. Hepsini aÅŸaÄŸÄ±daki hÃ¼creye ekleyin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### ğŸ¯ Meydan OkumanÄ±n Hedefleri\n",
    "\n",
    "#### ğŸ‹ï¸â€â™€ï¸ Gymnasium'a AÅŸinalÄ±k KazanÄ±n:\n",
    "Gymnasium'dan [CliffWalking ortamÄ±nÄ±](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) keÅŸfederek baÅŸlayacaÄŸÄ±z. OrtamlarÄ± yÃ¶netmek ve gÃ¶rselleÅŸtirmek iÃ§in kullanÄ±lan temel yÃ¶ntemleri anlayÄ±n. OrtamlarÄ± yÃ¼klemeyi ve sÄ±fÄ±rlamayÄ±, eylemler almayÄ± ve sonuÃ§larÄ± gÃ¶rselleÅŸtirmeyi Ã¶ÄŸrenin.\n",
    "\n",
    "#### ğŸ¤– Stable Baselines3 KullanmayÄ± Ã–ÄŸrenin:\n",
    "Bir pekiÅŸtirmeli Ã¶ÄŸrenme modelini kurmak ve eÄŸitmek iÃ§in Stable Baselines3 kÃ¼tÃ¼phanesini kullanÄ±n. Bir modeli yapÄ±landÄ±rmayÄ±, eÄŸitim parametrelerini ayarlamayÄ± ve Deep Q-Network (DQN) gibi popÃ¼ler bir algoritma kullanarak eÄŸitim sÃ¼recini baÅŸlatmayÄ± Ã¶ÄŸreneceksiniz.\n",
    "\n",
    "#### ğŸ“ˆ EÄŸitim PerformansÄ±nÄ± GÃ¶rselleÅŸtirin:\n",
    "Modelinizin eÄŸitim performansÄ±nÄ± izlemek ve Ã§izmek iÃ§in gÃ¼nlÃ¼k tutma ve TensorBoard gibi gÃ¶rselleÅŸtirme araÃ§larÄ±nÄ± uygulayÄ±n. EÄŸitim stratejilerinizin etkinliÄŸini deÄŸerlendirmek iÃ§in bÃ¶lÃ¼m baÅŸÄ±na Ã¶dÃ¼ller ve Ã¶ÄŸrenme eÄŸrileri gibi metrikleri analiz edin.\n",
    "\n",
    "#### ğŸ’¾ EÄŸitilmiÅŸ Bir Modeli YÃ¼kleyin ve DaÄŸÄ±tÄ±n:\n",
    "EÄŸitim sonrasÄ±nda, eÄŸitilmiÅŸ bir modeli kaydetmeyi ve daha sonra yÃ¼klemeyi Ã¶ÄŸrenin. Bu modeli, ajanÄ±n CliffWalking ortamÄ±yla etkileÅŸim kurduÄŸu bir simÃ¼lasyon Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanÄ±n ve Ã¶ÄŸrenilen politikalarÄ± ortamda etkili bir ÅŸekilde gezinmek iÃ§in uygulayÄ±n.\n",
    "\n",
    "#### ğŸ” DeÄŸerlendirin ve DÃ¼ÅŸÃ¼nÃ¼n:\n",
    "EÄŸitilmiÅŸ modelin ortam iÃ§indeki gerÃ§ek zamanlÄ± etkileÅŸimlerdeki performansÄ±nÄ± deÄŸerlendirin. Ã–ÄŸrenme sÃ¼reci ve ajanÄ±n davranÄ±ÅŸÄ± Ã¼zerine dÃ¼ÅŸÃ¼nÃ¼n, farklÄ± yapÄ±landÄ±rmalarÄ±n ve eÄŸitim sÃ¼relerinin sonuÃ§larÄ± nasÄ±l etkileyebileceÄŸini anlayÄ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 1: CliffWalking OrtamÄ±nÄ± KeÅŸfetmek\n",
    "\n",
    "Bu ilk gÃ¶revde, Gymnasium'dan [CliffWalking ortamÄ±nÄ±](https://gymnasium.farama.org/environments/toy_text/cliff_walking/) kuracaksÄ±nÄ±z.\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "\n",
    "#### 0. âš™ï¸ Paketi iÃ§e aktarÄ±n:\n",
    "Notebook'unuzun en Ã¼stÃ¼ndeki hÃ¼creye gymnasium'u iÃ§e aktarÄ±n. Kodumuzda `gym` olarak kullanabildiÄŸimizden emin olun.\n",
    "\n",
    "#### 1. ğŸ—‚ï¸ OrtamÄ± YÃ¼kleyin:\n",
    "- `CliffWalking` ortamÄ±nÄ± yÃ¼klemek iÃ§in `.make()` metodunu kullanÄ±n.  \n",
    "- OrtamÄ± gÃ¶rselleÅŸtirmek iÃ§in `render_mode`'u uygun ÅŸekilde ayarlayÄ±n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not: `gymnasium` dokÃ¼mantasyonu size `CliffWalking-v1` kullanmanÄ±zÄ± sÃ¶ylÃ¼yor, ancak bu henÃ¼z uyumluluk nedenleriyle kullandÄ±ÄŸÄ±mÄ±z `gymnasium` sÃ¼rÃ¼m 1.0.0'da bulunmuyor. Bunun yerine `CliffWalking-v0` kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea321476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CliffWalking-v1', 'CliffWalkingSlippery-v1', 'tabular/CliffWalking-v0']\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# CliffWalking var mÄ±?\n",
    "print([k for k in gym.registry.keys() if \"CliffWalking\" in k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "21fff3d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "CLIFF_ID = \"CliffWalking-v1\"   # biz bunu kullanacaÄŸÄ±z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca7323b",
   "metadata": {},
   "source": [
    "#### 2. ğŸ”„ OrtamÄ± BaÅŸlatÄ±n:\n",
    "OrtamÄ± baÅŸlangÄ±Ã§ durumuna getirmek iÃ§in `.reset()` metodunu Ã§aÄŸÄ±rÄ±n â€” onunla etkileÅŸim kurmadan Ã¶nce gereklidir. `.reset()` metodu bir durum dÃ¶ndÃ¼rÃ¼r: baÅŸlangÄ±Ã§ durumunu. Bunu bir deÄŸiÅŸkende saklayÄ±n ve yazdÄ±rÄ±n.\n",
    "\n",
    "> `UserWarning: pkg_resources is deprecated ...` uyarÄ±sÄ±nÄ± gÃ¶rebilirsiniz. Bunu gÃ¶rmezden gelebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 36 info: {'prob': 1}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(CLIFF_ID, render_mode=\"human\")  # pencere aÃ§ar\n",
    "obs, info = env.reset()\n",
    "print(\"obs:\", obs, \"info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`(36, {'prob': 1})` Ã§Ä±ktÄ±sÄ± ortamÄ±n durumu hakkÄ±nda iki bilgi parÃ§asÄ± saÄŸlar:\n",
    "\n",
    "**ğŸ”¢ Durum Ä°ndeksi (36)**: Bu sayÄ±, ajanÄ±n baÅŸlangÄ±Ã§ta ortam Ä±zgarasÄ± iÃ§indeki belirli konumunu temsil eder. Ã–rneÄŸin 'CliffWalking-v0' ortamÄ±nda, `36` indeksi ajanÄ±n bÃ¶lÃ¼mÃ¼ baÅŸlattÄ±ÄŸÄ± baÅŸlangÄ±Ã§ durumuna karÅŸÄ±lÄ±k gelir.\n",
    "\n",
    "**ğŸ² OlasÄ±lÄ±k Bilgisi ({'prob': 1})**: Bu sÃ¶zlÃ¼k, durum hakkÄ±nda, Ã¶zellikle geÃ§iÅŸ olasÄ±lÄ±ÄŸÄ± hakkÄ±nda ek ayrÄ±ntÄ±lar gÃ¶sterir. `'prob'` anahtarÄ±nÄ±n `1` deÄŸeri, bu duruma geÃ§iÅŸin 1 olasÄ±lÄ±ÄŸÄ±yla gerÃ§ekleÅŸtiÄŸini, yani kesin olduÄŸunu gÃ¶sterir. Bu mantÄ±klÄ±dÄ±r Ã§Ã¼nkÃ¼ ajan mutlaka bu durumda baÅŸlayacaktÄ±r."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ‘€ OrtamÄ± GÃ¶rselleÅŸtirin:\n",
    "Herhangi bir eylem almadan Ã¶nce ortamÄ± gÃ¶rÃ¼ntÃ¼lemek ve dÃ¼zenini anlamak iÃ§in `.render()` metodunu kullanÄ±n.\n",
    "\n",
    "Kurulumunuza baÄŸlÄ± olarak, yeni durum Ã¶nceki adÄ±mdan sonra zaten iÅŸlenmiÅŸ olabileceÄŸi iÃ§in bu adÄ±ma ihtiyacÄ±nÄ±z olmayabilir."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b893b558",
   "metadata": {},
   "source": [
    "#### 4. ğŸ§¹ OrtamÄ± KapatÄ±n:\n",
    "Ä°ÅŸiniz bittiÄŸinde kaynaklarÄ± serbest bÄ±rakmak iÃ§in `.close()` Ã§aÄŸÄ±rarak ortamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action: 0\n",
      "obs: 24 reward: -1 done: False info: {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "\n",
    "action = env.action_space.sample()\n",
    "print(\"action:\", action)\n",
    "\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "print(\"obs:\", obs, \"reward:\", reward, \"done:\", done, \"info:\", info)\n",
    "\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "  <summary markdown='span'>âš ï¸ <strong>macOS iÃ§in kapatma hakkÄ±nda not</strong></summary>\n",
    "  \n",
    "  `.close()` ortam penceresini kapatmayacak. Bu sorun deÄŸil, **aÃ§Ä±k bÄ±rakabilirsiniz**. Pencere sonraki adÄ±mlarda tekrar kullanÄ±lacak.\n",
    "\n",
    "  Meydan okumanÄ±n sonunda ve **sadece sonunda**, ortam penceresini kapatmak iÃ§in onu kendiniz zorla sonlandÄ±rmanÄ±z gerekecek:\n",
    "  1. Pencereyi bulun ve kapatmak iÃ§in kÄ±rmÄ±zÄ± dÃ¼ÄŸmeye tÄ±klayÄ±n. (Bu baÅŸarÄ±sÄ±z olacak.)\n",
    "  1. EkranÄ±nÄ±zÄ±n sol Ã¼st kÃ¶ÅŸesinde Apple sembolÃ¼ne tÄ±klayÄ±n.\n",
    "  1. `Force Quit`'i (Zorla Ã‡Ä±kÄ±ÅŸ) seÃ§in.\n",
    "  1. Ä°ÅŸlem listesinde `python (Not Responding)` iÅŸlemini bulun. Onu seÃ§in.\n",
    "  1. `Force Quit` dÃ¼ÄŸmesine tÄ±klayÄ±n.\n",
    "\n",
    "  Bu Ã§ekirdeÄŸinizi sonlandÄ±racaÄŸÄ±ndan, **bunu sadece meydan okumanÄ±n sonunda yapÄ±n**.\n",
    "\n",
    "  Bu durum Jupyter Notebook iÃ§inde Ã§alÄ±ÅŸtÄ±rdÄ±ÄŸÄ±mÄ±z iÃ§in oluyor. Kodunuzu bir `.py` dosyasÄ±na taÅŸÄ±rsanÄ±z, bu davranÄ±ÅŸÄ± gÃ¶rmezsiniz.\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 2: Ortamla EtkileÅŸim Kurmak\n",
    "\n",
    "Åimdi ortamÄ± tekrar yÃ¼kleyelim, rastgele bir adÄ±m atalÄ±m ve sonucu gÃ¶sterelim. Bu, ortamlarla etkileÅŸim kurmak iÃ§in temel metotlarÄ± tanÄ±tacak â€” ajanlarÄ±n nasÄ±l hareket ettiÄŸini ve geri bildirim aldÄ±ÄŸÄ±nÄ± anlamanÄ±za yardÄ±mcÄ± olacak."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. ğŸ—‚ï¸ OrtamÄ± YÃ¼kleyin\n",
    "- OrtamÄ± yÃ¼kleyin.\n",
    "- BaÅŸlangÄ±Ã§ durumuna sÄ±fÄ±rlayÄ±n.\n",
    "- BaÅŸlangÄ±Ã§ durumunu yazdÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 36 info: {'prob': 1}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# OrtamÄ± yÃ¼kle (gÃ¶rsel pencere istiyorsan \"human\", istemiyorsan None yap)\n",
    "env = gym.make(\"CliffWalking-v1\", render_mode=\"human\")\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ durumuna sÄ±fÄ±rla\n",
    "obs, info = env.reset()\n",
    "\n",
    "# BaÅŸlangÄ±Ã§ durumunu yazdÄ±r\n",
    "print(\"obs:\", obs, \"info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ğŸ² Bir Eylem Ã–rnekleyin:\n",
    "- OrtamÄ±n eylem alanÄ±ndan rastgele bir eylem seÃ§mek iÃ§in `.action_space.sample()` kullanÄ±n â€” keÅŸfi simÃ¼le eder.\n",
    "- Bunu bir deÄŸiÅŸkende kaydedin.\n",
    "- Eylemin tÃ¼rÃ¼nÃ¼ ve deÄŸerini inceleyin.\n",
    "- HÃ¼creyi birkaÃ§ kez Ã§alÄ±ÅŸtÄ±rÄ±n. Hangi deÄŸerleri gÃ¶rÃ¼yorsunuz? Bunlar neyi temsil ediyor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action type: <class 'numpy.int64'>\n",
      "action value: 2\n"
     ]
    }
   ],
   "source": [
    "# Rastgele bir eylem seÃ§\n",
    "action = env.action_space.sample()\n",
    "\n",
    "print(\"action type:\", type(action))\n",
    "print(\"action value:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. ğŸ¦¶ Eylemi GerÃ§ekleÅŸtirin:\n",
    "- `.step()` kullanarak eylemi uygulayÄ±n.  \n",
    "- Bu ÅŸunlarÄ± dÃ¶ndÃ¼rÃ¼r:  \n",
    "  - Yeni durum  \n",
    "  - Ã–dÃ¼l  \n",
    "  - Bir `done` bayraÄŸÄ± (bÃ¶lÃ¼m bitmiÅŸ mi deÄŸil mi)  \n",
    "  - Ek bilgi (varsa)\n",
    "- \"tuple unpacking\" kullanarak bu dÃ¶nen deÄŸerlerin her birini bir deÄŸiÅŸkende kaydedin. OnlarÄ± yazdÄ±rÄ±n. Yeni durum, Ã¶dÃ¼l ve done dÃ¶nÃ¼ÅŸ deÄŸerlerini anlÄ±yor musunuz?\n",
    "- Bu **sÄ±fÄ±rla > eylem > adÄ±m** dizisini birkaÃ§ kez Ã§alÄ±ÅŸtÄ±rÄ±n ve farklÄ± sonuÃ§larÄ± kontrol edin.\n",
    "\n",
    "<details>\n",
    "  <summary markdown='span'>\n",
    "  ğŸ’¡ Tuple unpacking?\n",
    "  </summary>\n",
    "\n",
    "  Bir fonksiyon tuple dÃ¶ndÃ¼rÃ¼rse, tuple'Ä±n farklÄ± elemanlarÄ±nÄ± hemen farklÄ± deÄŸiÅŸkenlere kaydedebilirsiniz.\n",
    "\n",
    "  Ã–rnek:\n",
    "\n",
    "  Åu fonksiyonunuz olduÄŸunu dÃ¼ÅŸÃ¼nÃ¼n:\n",
    "\n",
    "  ```python\n",
    "  def surface_and_circumference(a, b):\n",
    "    \"\"\"UzunluÄŸu `a` ve geniÅŸliÄŸi `b` olan dikdÃ¶rtgenin \n",
    "    alanÄ±nÄ± ve Ã§evresini dÃ¶ndÃ¼rÃ¼r.\"\"\"\n",
    "    return a*b, 2*a + 2*b\n",
    "  ```\n",
    "\n",
    "  Bunun yerine:\n",
    "\n",
    "  ```python\n",
    "  result = surface_and_circumference(4, 2)\n",
    "  surface = result[0]\n",
    "  circumference = result[1]\n",
    "  ```\n",
    "\n",
    "  Hemen ÅŸunu yapabilirsiniz:\n",
    "\n",
    "  ```python\n",
    "  surface, circumference = surface_and_circumference(4, 2)\n",
    "  ```\n",
    "\n",
    "  Kodunuzun geri kalanÄ±nda sadece surface kullanacaksanÄ±z, diÄŸer dÃ¶nÃ¼ÅŸ deÄŸerleri iÃ§in `_` kullanmak yaygÄ±n bir alÄ±ÅŸkanlÄ±ktÄ±r. Bu, diÄŸer programcÄ±lara kalan deÄŸerleri attÄ±ÄŸÄ±nÄ±zÄ±n bir iÅŸaretidir.\n",
    "\n",
    "  Ã–rnek:\n",
    "  \n",
    "  ```python\n",
    "  surface, _ = surface_and_circumference(4, 2)\n",
    "  # Buradan sonra sadece surface'a ihtiyaÃ§ duyan kod gelir\n",
    "  ```\n",
    "\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obs: 36 reward: -1 done: False info: {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Eylemi uygula\n",
    "obs, reward, terminated, truncated, info = env.step(action)\n",
    "done = terminated or truncated\n",
    "\n",
    "print(\"obs:\", obs, \"reward:\", reward, \"done:\", done, \"info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ğŸ“ Yeni Durum**: Bu sayÄ±, belirtilen eylemi gerÃ§ekleÅŸtirdikten sonra ajanÄ±n durumunu temsil eder. `CliffWalking` ortamÄ±nda durum, ajanÄ±n hareket ettiÄŸi ortam Ä±zgarasÄ±ndaki belirli bir konuma karÅŸÄ±lÄ±k gelir.\n",
    "\n",
    "**ğŸ’¸ Ã–dÃ¼l**: Ã–dÃ¼l deÄŸeri, ajanÄ±n eyleminin sonucu olarak ortam tarafÄ±ndan verilen anlÄ±k geri bildirimi gÃ¶sterir. BirÃ§ok Ä±zgara tabanlÄ± ortamda, bunun gibi negatif bir Ã¶dÃ¼l genellikle bir cezayÄ± ifade eder ve alÄ±nan eylemin optimal olmayabileceÄŸini veya diÄŸer stratejileri teÅŸvik etmek iÃ§in cezalandÄ±rÄ±ldÄ±ÄŸÄ±nÄ± gÃ¶sterir.\n",
    "\n",
    "**ğŸš¦ Done**: Boolean deÄŸer bÃ¶lÃ¼mÃ¼n bitip bitmediÄŸini gÃ¶sterir. Bu durumda, `False` bÃ¶lÃ¼mÃ¼n hala devam ettiÄŸi ve ajanÄ±n bÃ¶lÃ¼mÃ¼ sona erdirecek terminal bir duruma (hedef veya Ã§ukur gibi) ulaÅŸmadÄ±ÄŸÄ± anlamÄ±na gelir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ‘ï¸ Sonucu GÃ¶rselleÅŸtirin:\n",
    "- Eylem sonrasÄ±nda ortamÄ± gÃ¶rselleÅŸtirmek iÃ§in tekrar `.render()` Ã§aÄŸÄ±rÄ±n â€” ve durumun nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶rÃ¼n.\n",
    "- Render penceresi gÃ¶rmÃ¼yorsanÄ±z, muhtemelen diÄŸer pencerelerinizin arkasÄ±nda gizli veya baÅŸka bir masaÃ¼stÃ¼nde gÃ¶steriliyor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ğŸ§¹ OrtamÄ± KapatÄ±n:\n",
    "Ä°ÅŸiniz bittiÄŸinde kaynaklarÄ± serbest bÄ±rakmak iÃ§in `.close()` Ã§aÄŸÄ±rarak ortamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment closed âœ…\n"
     ]
    }
   ],
   "source": [
    "# 4) OrtamÄ± kapat\n",
    "env.close()\n",
    "print(\"Environment closed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 3: Ortamla YÃ¶nlendirilmiÅŸ EtkileÅŸim\n",
    "\n",
    "Bu sefer rastgele bir eylem seÃ§mek yerine kasÄ±tlÄ± bir adÄ±m atacaksÄ±nÄ±z â€” **YUKARI** hareket edeceÄŸiz. Bu, pekiÅŸtirmeli Ã¶ÄŸrenmede amaÃ§lÄ± karar verme fikrini pekiÅŸtirir.\n",
    "\n",
    "#### 1. ğŸš€ BaÅŸlatÄ±n ve Ä°lk Durumu YazdÄ±rÄ±n:\n",
    "- OrtamÄ± yÃ¼kleyin ve baÅŸlangÄ±Ã§ durumunu almak iÃ§in `.reset()` Ã§aÄŸÄ±rÄ±n.  \n",
    "- Herhangi bir eylem almadan Ã¶nce baÅŸlangÄ±Ã§ durumunu gÃ¶rÃ¼ntÃ¼lemek iÃ§in `print()` kullanÄ±n.\n",
    "- Åu Ã§Ä±ktÄ±yÄ± almalÄ±sÄ±nÄ±z `(36, {'prob': 1})`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial obs: 36 info: {'prob': 1}\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "# 1) OrtamÄ± yÃ¼kle\n",
    "env = gym.make(\"CliffWalking-v1\", render_mode=\"human\")\n",
    "\n",
    "# 2) BaÅŸlangÄ±Ã§ durumuna sÄ±fÄ±rla\n",
    "obs, info = env.reset()\n",
    "\n",
    "# 3) BaÅŸlangÄ±Ã§ durumunu yazdÄ±r\n",
    "print(\"initial obs:\", obs, \"info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. â¬†ï¸ 'YUKARI' Eylemini Belirleyin ve GerÃ§ekleÅŸtirin\n",
    "\n",
    "- `.action_space`'i ve dokÃ¼mantasyonu kontrol ederek **'YUKARI'** eylemi iÃ§in indeksi bulun.  \n",
    "- Bu eylemi gerÃ§ekleÅŸtirmek iÃ§in `.step(action_index)` kullanÄ±n.  \n",
    "- ÅunlarÄ± gÃ¶rmek iÃ§in sonucu yazdÄ±rÄ±n:\n",
    "  - Yeni durum  \n",
    "  - AlÄ±nan Ã¶dÃ¼l  \n",
    "  - BÃ¶lÃ¼mÃ¼n bitip bitmediÄŸi (`done` bayraÄŸÄ±)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_space: Discrete(4)\n",
      "UP action index: 0\n",
      "new obs: 24\n",
      "reward: -1\n",
      "done: False\n",
      "info: {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "# Aksiyon uzayÄ±nÄ± incele\n",
    "print(\"action_space:\", env.action_space)\n",
    "\n",
    "# ToyText ortamlarÄ±nda genelde ÅŸu mapping vardÄ±r:\n",
    "# 0=UP, 1=RIGHT, 2=DOWN, 3=LEFT\n",
    "UP = 0\n",
    "\n",
    "# UP eylemini uygula\n",
    "new_obs, reward, terminated, truncated, info = env.step(UP)\n",
    "done = terminated or truncated\n",
    "\n",
    "print(\"UP action index:\", UP)\n",
    "print(\"new obs:\", new_obs)\n",
    "print(\"reward:\", reward)\n",
    "print(\"done:\", done)\n",
    "print(\"info:\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ–¼ï¸ OrtamÄ± GÃ¶rselleÅŸtirin ve KapatÄ±n\n",
    "\n",
    "- Eylem sonrasÄ±nda ortamÄ± gÃ¶rselleÅŸtirmek iÃ§in `.render()` kullanÄ±n â€” durumun nasÄ±l deÄŸiÅŸtiÄŸini gÃ¶rÃ¼n.\n",
    "- ArdÄ±ndan ortamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatmak ve sistem kaynaklarÄ±nÄ± serbest bÄ±rakmak iÃ§in `.close()` Ã§aÄŸÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment closed âœ…\n"
     ]
    }
   ],
   "source": [
    "# OrtamÄ± gÃ¶rselleÅŸtir (bazÄ± kurulumlarda zaten pencere gÃ¼ncellenmiÅŸ olur)\n",
    "env.render()\n",
    "\n",
    "# OrtamÄ± kapat\n",
    "env.close()\n",
    "print(\"Environment closed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### BÃ¶lÃ¼m 4: Bir RL AlgoritmasÄ± EÄŸitmek\n",
    "\n",
    "Ä°lk olarak, `CliffWalking-v0` ortamÄ±nÄ± Stable Baselines3 modelleriyle eÄŸitim iÃ§in hazÄ±rlayacaksÄ±nÄ±z. Bu, RL algoritmalarÄ±yla uyumluluÄŸu saÄŸlamak iÃ§in uygun kurulum ve sarmalama iÃ§erir.\n",
    "\n",
    "#### ğŸ§± 1. OrtamÄ± YÃ¼kleyin:\n",
    "- `CliffWalking-v0` ortamÄ±nÄ± oluÅŸturmak iÃ§in `gym.make()` kullanÄ±n.  \n",
    "- `render_mode='human'` ayarlayÄ±n â†’ EtkileÅŸim sÄ±rasÄ±nda gÃ¶rsel geri bildirim saÄŸlar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b4792b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obs space: Discrete(48)\n",
      "Action space: Discrete(4)\n"
     ]
    }
   ],
   "source": [
    "# 1) OrtamÄ± oluÅŸtur (izlemek iÃ§in human)\n",
    "env = gym.make(\"CliffWalking-v1\", render_mode=\"human\")\n",
    "\n",
    "print(\"Obs space:\", env.observation_space)\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ§µ 2. OrtamÄ± SarÄ±n\n",
    "\n",
    "OrtamÄ±nÄ±zÄ± [`DummyVecEnv`](https://stable-baselines3.readthedocs.io/en/v1.0/guide/vec_envs.html#dummyvecenv) ile sarmak kÃ¼Ã§Ã¼k ama kritik bir adÄ±mdÄ±r â€” RL kurulumunuzun Stable Baselines3 ile sorunsuz Ã§alÄ±ÅŸmasÄ±nÄ± saÄŸlar.\n",
    "\n",
    "Devam edin ve ortamÄ± Stable Baselines3'ten [`DummyVecEnv`](https://stable-baselines3.readthedocs.io/en/v1.0/guide/vec_envs.html#dummyvecenv) kullanarak sarÄ±n.  ğŸ‘‡"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapped env: <stable_baselines3.common.vec_env.dummy_vec_env.DummyVecEnv object at 0x30a8a5e80>\n"
     ]
    }
   ],
   "source": [
    "# 2) SB3 uyumluluÄŸu iÃ§in sarmala (tek ortam bile olsa)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "print(\"Wrapped env:\", env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ§µ `DummyVecEnv` HakkÄ±nda Daha Fazla\n",
    "\n",
    "`DummyVecEnv`, Gym ortamlarÄ±nÄ± vektÃ¶rleÅŸtirerek RL modelleriyle uyumlu hale getiren Stable Baselines3'ten bir sarmalayÄ±cÄ±dÄ±r.\n",
    "\n",
    "#### âš™ï¸ Ne Yapar?\n",
    "\n",
    "- **ğŸ“¦ API'yi StandartlaÅŸtÄ±rÄ±r**  \n",
    "  OrtamÄ±n Stable Baselines3 algoritmalarÄ± iÃ§in beklenen formatla eÅŸleÅŸmesini saÄŸlar.\n",
    "\n",
    "- **ğŸ“Š Toplu Ä°ÅŸlemeyi EtkinleÅŸtirir**  \n",
    "  Tek bir ortamla bile etkileÅŸimler bir toplu gibi iÅŸlenir â€” daha sonra `SubprocVecEnv` gibi araÃ§larla Ã¶lÃ§ekleme iÃ§in gereklidir.\n",
    "\n",
    "- **ğŸ”— UyumluluÄŸu SaÄŸlar**  \n",
    "  `reset()` ve `step()`'i eÄŸitim dÃ¶ngÃ¼leri iÃ§inde doÄŸru Ã§alÄ±ÅŸacak ÅŸekilde sarar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. SB3'den Bir DQN Modeli BaÅŸlatÄ±n\n",
    "\n",
    "ArtÄ±k ortamÄ±nÄ±z hazÄ±r olduÄŸuna gÃ¶re, modeli baÅŸlatmanÄ±z gerekir. Bu durumda, Stable Baselines3 kullanarak bir Deep Q-Network (DQN) yapÄ±landÄ±racaksÄ±nÄ±z. DQN, Q-deÄŸerlerini tahmin etmek iÃ§in bir sinir aÄŸÄ± kullanÄ±r â€” belirli bir durumda her eylem iÃ§in beklenen Ã¶dÃ¼lÃ¼ tahmin eder.\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "- `stable_baselines3`'den `DQN`'i iÃ§e aktarÄ±n. Bunu notebook'un en Ã¼stÃ¼ndeki hÃ¼creye ekleyin.\n",
    "- AÅŸaÄŸÄ±daki parametrelerle DQN modelinin bir Ã¶rneÄŸini baÅŸlatÄ±n:\n",
    "    - `'MlpPolicy'` kullanÄ±n â€” gÃ¶zlemleri eylemlere eÅŸleyen temel bir sinir aÄŸÄ±.  \n",
    "    - `env`'i ortamÄ±nÄ±za ve ayrÄ±ntÄ±lÄ± eÄŸitim gÃ¼nlÃ¼kleri iÃ§in `verbose=1`'e ayarlayÄ±n.\n",
    "    - EÄŸitim metriklerini izlemek iÃ§in `tensorboard_log` parametresi ekleyin. Bunu daha sonra TensorBoard ile eÄŸitimi takip etmek iÃ§in kullanacaÄŸÄ±z."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "DQN model initialized âœ…\n"
     ]
    }
   ],
   "source": [
    "# TensorBoard log klasÃ¶rÃ¼\n",
    "log_dir = \"./dqn_cliff_tensorboard\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "# 3) DQN modelini baÅŸlat\n",
    "model = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env,\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_dir,\n",
    ")\n",
    "\n",
    "print(\"DQN model initialized âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. ğŸ‹ï¸â€â™‚ï¸ DQN Modelini EÄŸitin ve Kaydedin\n",
    "\n",
    "DQN modelinizi eÄŸitme ve gelecekte kullanmak Ã¼zere kaydetme zamanÄ± ğŸ¥³\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "\n",
    "#### â±ï¸ EÄŸitim AdÄ±m SayÄ±sÄ±nÄ± AyarlayÄ±n\n",
    "- Modeli ne kadar sÃ¼re eÄŸiteceÄŸinizi (ortamla etkileÅŸim sayÄ±sÄ± cinsinden) tanÄ±mlayÄ±n.  \n",
    "- Åimdilik ÅŸunu kullanÄ±n: `total_timesteps = 1000`\n",
    "\n",
    "#### ğŸ§  Modeli EÄŸitin\n",
    "- EÄŸitimi baÅŸlatmak iÃ§in DQN modelinizde `.learn()` Ã§aÄŸÄ±rÄ±n.  \n",
    "- Model, geri bildirimlere dayanarak zaman iÃ§inde politikasÄ±nÄ± geliÅŸtirecek.\n",
    "\n",
    "#### ğŸ’¾ EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "- Modeli diske kaydetmek iÃ§in `.save()` kullanÄ±n.  \n",
    "- DoÄŸru kaydedildiÄŸini doÄŸrulamak iÃ§in dosya yolunu yazdÄ±rÄ±n.\n",
    "\n",
    "ğŸ¥ EÄŸitim sÄ±rasÄ±nda iÅŸlenmiÅŸ ortamdaki adÄ±mlarÄ± takip edebilirsiniz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_cliff_tensorboard/DQN_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e80fc7e518948fdb1996e43ccdb8ee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved âœ… -> dqn_cliffwalking_1k.zip\n"
     ]
    }
   ],
   "source": [
    "# 4) EÄŸitim adÄ±m sayÄ±sÄ±\n",
    "total_timesteps = 1000\n",
    "\n",
    "# Modeli eÄŸit (progress_bar=True destekleniyorsa bar gÃ¶sterir)\n",
    "model.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "\n",
    "# Kaydetme yolu\n",
    "model_path = \"dqn_cliffwalking_1k\"\n",
    "\n",
    "# Modeli kaydet\n",
    "model.save(model_path)\n",
    "\n",
    "print(f\"Model saved âœ… -> {model_path}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dc51d9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env closed âœ…\n"
     ]
    }
   ],
   "source": [
    "# EÄŸitim sonrasÄ± ortamÄ± kapat\n",
    "env.close()\n",
    "print(\"Env closed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tebrikler, ilk RL modelinizi eÄŸittiniz!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BÃ¶lÃ¼m 5: GÃ¶rselleÅŸtirme Olmadan Verimli EÄŸitim\n",
    "\n",
    "Bu bÃ¶lÃ¼mde, ortamÄ± hazÄ±rlama, modeli yÃ¼kleme ve eÄŸitme sÃ¼recinin tamamÄ±ndan geÃ§eceksiniz, ancak gÃ¶rsel iÅŸleme olmadan. EÄŸitim aÅŸamasÄ±nda gÃ¶rselleÅŸtirmeyi devre dÄ±ÅŸÄ± bÄ±rakmak, hesaplama yÃ¼kÃ¼nÃ¼ azalttÄ±ÄŸÄ± iÃ§in eÄŸitim sÃ¼recini Ã¶nemli Ã¶lÃ§Ã¼de hÄ±zlandÄ±rabilir.\n",
    "\n",
    "#### ğŸ“ Takip Edilecek AdÄ±mlar:\n",
    "\n",
    "#### ğŸ§± 1. OrtamÄ± YÃ¼kleyin ve HazÄ±rlayÄ±n\n",
    "- GÃ¶rsel Ã§Ä±ktÄ±yÄ± devre dÄ±ÅŸÄ± bÄ±rakmak iÃ§in `render_mode=None` kullanÄ±n.\n",
    "\n",
    "#### âš™ï¸ 2. DQN Modelini YapÄ±landÄ±rÄ±n ve BaÅŸlatÄ±n\n",
    "- Daha Ã¶nce olduÄŸu gibi `'MlpPolicy'` ve `DummyVecEnv` kullanÄ±n.  \n",
    "- GÃ¼nlÃ¼k Ã§Ä±ktÄ±sÄ± istiyorsanÄ±z `verbose=1`'i koruyun.\n",
    "- TensorBoard ile eÄŸitimi takip edebilmemiz iÃ§in bir TensorBoard gÃ¼nlÃ¼kleme konumu `./dqn_tensorboard` ekleyeceÄŸiz.\n",
    "\n",
    "#### â±ï¸ 3. EÄŸitim Parametrelerini AyarlayÄ±n ve EÄŸitin\n",
    "- Daha iyi Ã¶ÄŸrenme iÃ§in adÄ±m sayÄ±sÄ±nÄ± artÄ±rÄ±n (Ã¶rn. `total_timesteps = 100_000`).  \n",
    "- EÄŸitimi baÅŸlatmak iÃ§in `.learn()` Ã§aÄŸÄ±rÄ±n.\n",
    "\n",
    "#### ğŸ’¾ 4. EÄŸitilmiÅŸ Modeli Kaydedin\n",
    "- Modelinizi saklamak iÃ§in `.save(\"dqn_cliffwalking_fast\")` kullanÄ±n.\n",
    "\n",
    "Bu adÄ±mlarÄ± izleyerek, ortamÄ± gÃ¶rselleÅŸtirmenin ek hesaplama yÃ¼kÃ¼ olmadan verimli bir ÅŸekilde bir DQN modeli eÄŸitir ve kaydedersiniz. Bu yaklaÅŸÄ±m Ã¶zellikle karmaÅŸÄ±k modeller eÄŸitirken veya sÄ±nÄ±rlÄ± hesaplama kaynaklarÄ± kullanÄ±rken faydalÄ±dÄ±r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gymnasium as gym\n",
    "\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1fb79fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast env ready âœ…\n"
     ]
    }
   ],
   "source": [
    "# 1) GÃ¶rsel yok: daha hÄ±zlÄ± eÄŸitim\n",
    "env_fast = gym.make(\"CliffWalking-v1\", render_mode=None)\n",
    "\n",
    "# 2) SB3 uyumluluÄŸu iÃ§in sar\n",
    "env_fast = DummyVecEnv([lambda: env_fast])\n",
    "\n",
    "print(\"Fast env ready âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ba9cf39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "DQN fast model initialized âœ…\n"
     ]
    }
   ],
   "source": [
    "log_dir_fast = \"./dqn_cliff_tensorboard_fast\"\n",
    "os.makedirs(log_dir_fast, exist_ok=True)\n",
    "\n",
    "model_fast = DQN(\n",
    "    policy=\"MlpPolicy\",\n",
    "    env=env_fast,\n",
    "    verbose=1,\n",
    "    tensorboard_log=log_dir_fast,\n",
    ")\n",
    "\n",
    "print(\"DQN fast model initialized âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "90f7164d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: macOS-26.2-arm64-arm-64bit Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.7.1\n",
      "- PyTorch: 2.9.1\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.2\n",
      "- Gymnasium: 1.2.3\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: macOS-26.2-arm64-arm-64bit Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.7.1\n",
      "- PyTorch: 2.9.1\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.2\n",
      "- Gymnasium: 1.2.3\n",
      "\n",
      "Loaded dqn_cliffwalking_1k and ready to continue âœ…\n"
     ]
    }
   ],
   "source": [
    "log_dir_fast = \"./dqn_cliff_tensorboard_fast\"\n",
    "os.makedirs(log_dir_fast, exist_ok=True)\n",
    "\n",
    "model_fast = DQN.load(\"dqn_cliffwalking_1k\", env=env_fast, print_system_info=True)\n",
    "model_fast.tensorboard_log = log_dir_fast  # loglarÄ± buraya yazsÄ±n\n",
    "\n",
    "print(\"Loaded dqn_cliffwalking_1k and ready to continue âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "862d110a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging to ./dqn_cliff_tensorboard_fast/DQN_1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "405edca10c01408ba361a5b4e2d1e2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 4        |\n",
      "|    fps              | 4551     |\n",
      "|    time_elapsed     | 11       |\n",
      "|    total_timesteps  | 54454    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 5.79e-05 |\n",
      "|    n_updates        | 13813    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 4546     |\n",
      "|    time_elapsed     | 12       |\n",
      "|    total_timesteps  | 57584    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000406 |\n",
      "|    n_updates        | 14595    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 4513     |\n",
      "|    time_elapsed     | 16       |\n",
      "|    total_timesteps  | 73020    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 6.43e-05 |\n",
      "|    n_updates        | 18454    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 16       |\n",
      "|    fps              | 4509     |\n",
      "|    time_elapsed     | 17       |\n",
      "|    total_timesteps  | 76705    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000308 |\n",
      "|    n_updates        | 19376    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 20       |\n",
      "|    fps              | 4496     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 81890    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000195 |\n",
      "|    n_updates        | 20672    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 24       |\n",
      "|    fps              | 4490     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 83637    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000214 |\n",
      "|    n_updates        | 21109    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 4491     |\n",
      "|    time_elapsed     | 18       |\n",
      "|    total_timesteps  | 85272    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 7.24e-05 |\n",
      "|    n_updates        | 21517    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 32       |\n",
      "|    fps              | 4483     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 87483    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 9.1e-05  |\n",
      "|    n_updates        | 22070    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 4479     |\n",
      "|    time_elapsed     | 19       |\n",
      "|    total_timesteps  | 88807    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000114 |\n",
      "|    n_updates        | 22401    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 4475     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 91915    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000122 |\n",
      "|    n_updates        | 23178    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 44       |\n",
      "|    fps              | 4470     |\n",
      "|    time_elapsed     | 20       |\n",
      "|    total_timesteps  | 93351    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000203 |\n",
      "|    n_updates        | 23537    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 48       |\n",
      "|    fps              | 4463     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 95833    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.000535 |\n",
      "|    n_updates        | 24158    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 4456     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 97752    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.64e-05 |\n",
      "|    n_updates        | 24637    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 56       |\n",
      "|    fps              | 4448     |\n",
      "|    time_elapsed     | 22       |\n",
      "|    total_timesteps  | 99316    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.006    |\n",
      "|    n_updates        | 25028    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved âœ… -> dqn_cliffwalking_fast.zip\n"
     ]
    }
   ],
   "source": [
    "total_timesteps = 100_000\n",
    "\n",
    "model_fast.learn(total_timesteps=total_timesteps, progress_bar=True)\n",
    "\n",
    "model_fast_path = \"dqn_cliffwalking_fast\"\n",
    "model_fast.save(model_fast_path)\n",
    "\n",
    "print(f\"Model saved âœ… -> {model_fast_path}.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "33ce8829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fast env closed âœ…\n"
     ]
    }
   ],
   "source": [
    "env_fast.close()\n",
    "print(\"Fast env closed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ“Š EÄŸitim GÃ¼nlÃ¼k Metriklerini Anlamak\n",
    "\n",
    "Bir pekiÅŸtirmeli Ã¶ÄŸrenme modelini eÄŸitirken, Ã§eÅŸitli metrikler Ã¶ÄŸrenme ilerlemesini ve performansÄ±nÄ± izlemeye yardÄ±mcÄ± olur.\n",
    "\n",
    "#### ğŸ² Rollout Metrikleri\n",
    "\n",
    "- **exploration_rate** â†’ Rastgele bir eylem alma olasÄ±lÄ±ÄŸÄ±.  \n",
    "  - YÃ¼ksek = daha fazla keÅŸif  \n",
    "  - DÃ¼ÅŸÃ¼k = daha fazla istismar  \n",
    "\n",
    "#### â±ï¸ Zaman ile Ä°lgili Metrikler\n",
    "\n",
    "- **episodes** â†’ Tamamlanan bÃ¶lÃ¼m sayÄ±sÄ±.  \n",
    "- **fps** â†’ Saniye baÅŸÄ±na kare (eÄŸitimin ne kadar hÄ±zlÄ± Ã§alÄ±ÅŸtÄ±ÄŸÄ±).  \n",
    "- **time_elapsed** â†’ EÄŸitim baÅŸladÄ±ÄŸÄ±ndan beri geÃ§en toplam sÃ¼re (saniye).  \n",
    "- **total_timesteps** â†’ Ortamda atÄ±lan toplam adÄ±m sayÄ±sÄ±.\n",
    "\n",
    "#### ğŸ§  EÄŸitim Metrikleri\n",
    "\n",
    "- **learning_rate** â†’ Model aÄŸÄ±rlÄ±klarÄ±na yapÄ±lan gÃ¼ncellemelerin boyutu.  \n",
    "  - DÃ¼ÅŸÃ¼k = daha yavaÅŸ ama daha kararlÄ± Ã¶ÄŸrenme  \n",
    "- **loss** â†’ Modelin tahmin hatasÄ±.  \n",
    "  - Azalan kayÄ±p = Ã¶ÄŸrenme Ã§alÄ±ÅŸÄ±yor  \n",
    "- **n_updates** â†’ Modelin aÄŸÄ±rlÄ±klarÄ±nÄ± kaÃ§ kez gÃ¼ncellediÄŸi.\n",
    "\n",
    "#### ğŸ” NasÄ±l YorumlanÄ±r\n",
    "\n",
    "- **â¬‡ï¸ exploration_rate** â†’ Ajan keÅŸiften Ã¶ÄŸrenilmiÅŸ davranÄ±ÅŸa geÃ§iyor.  \n",
    "- **âš¡ YÃ¼ksek fps** â†’ EÄŸitim verimli Ã§alÄ±ÅŸÄ±yor.  \n",
    "- **ğŸ“‰ Azalan kayÄ±p** â†’ Model geliÅŸiyor ve daha az hata yapÄ±yor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ–¥ï¸ TensorBoard'u BaÅŸlatÄ±n\n",
    "\n",
    "TensorBoard eÄŸitimin nasÄ±l gittiÄŸini takip etmenizi saÄŸlar. EtkileÅŸimli pano Ã¶ÄŸrenme eÄŸrilerini, Ã¶dÃ¼l trendlerini ve daha fazlasÄ±nÄ± gÃ¶sterir. \n",
    "\n",
    "TensorBoard kullanmak iÃ§in:\n",
    "\n",
    "1. Bir terminal penceresi aÃ§Ä±n.\n",
    "1. Meydan okuma klasÃ¶rÃ¼nde olduÄŸunuzdan emin olun!\n",
    "1. Bu komutu Ã§alÄ±ÅŸtÄ±rÄ±n (eÄŸitim sÄ±rasÄ±nda ayarladÄ±ÄŸÄ±nÄ±z yol ile deÄŸiÅŸtirmeniz gerekebilir):\n",
    "   ```bash\n",
    "   tensorboard --logdir=./dqn_cliff_tensorboard/\n",
    "   ```\n",
    "1. Terminalde gÃ¶sterilecek baÄŸlantÄ±yÄ± takip edin (muhtemelen `localhost:6006`).\n",
    "\n",
    "Modeliniz hala eÄŸitilirken TensorBoard'u baÅŸlatabilirsiniz: amacÄ± eÄŸitimin nasÄ±l gittiÄŸini takip etmektir. EÄŸitimin baÅŸlangÄ±cÄ±nda `No dashboards are active for the current data set.` uyarÄ±sÄ± alabilirsiniz. **Biraz sabÄ±rlÄ± olun: ilk bÃ¶lÃ¼m bitene kadar hiÃ§bir ÅŸey gÃ¶rmeyeceksiniz.**\n",
    "\n",
    "TensorBoard'u notebook'unuzun iÃ§inde aÃ§mak da mÃ¼mkÃ¼n:\n",
    "\n",
    "```bash\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir=./dqn_cliff_tensorboard/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ“¦ BÃ¶lÃ¼m 6: EÄŸitilmiÅŸ Bir Model KullanÄ±n\n",
    "\n",
    "SÄ±fÄ±rdan tekrar eÄŸitmek yerine, ajanÄ±nÄ±zÄ±n nasÄ±l davrandÄ±ÄŸÄ±nÄ± hÄ±zlÄ±ca gÃ¶zlemlemek iÃ§in az Ã¶nce eÄŸittiÄŸiniz modeli yÃ¼kleyebilirsiniz.  \n",
    "Alternatif olarak, mevcut olduÄŸu takdirde daha gÃ¼Ã§lÃ¼ Ã¶nceden eÄŸitilmiÅŸ bir model yÃ¼kleyebilirsiniz.\n",
    "\n",
    "#### ğŸ“¥ 1. Ã–nceden EÄŸitilmiÅŸ Modeli YÃ¼kleyin\n",
    "\n",
    "- Modeli diskten yÃ¼klemek iÃ§in `.load(\"Ã¶nceden_eÄŸitilmiÅŸ_modelinizin_yolu\")` kullanÄ±n.  \n",
    "- Yolu kaydettiÄŸiniz modelin gerÃ§ek konumuyla deÄŸiÅŸtirin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50d3badc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: macOS-26.2-arm64-arm-64bit Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.7.1\n",
      "- PyTorch: 2.9.1\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.2\n",
      "- Gymnasium: 1.2.3\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: macOS-26.2-arm64-arm-64bit Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.7.1\n",
      "- PyTorch: 2.9.1\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.2\n",
      "- Gymnasium: 1.2.3\n",
      "\n",
      "Model loaded âœ…\n"
     ]
    }
   ],
   "source": [
    "model_path = \"dqn_cliffwalking_fast\"\n",
    "model = DQN.load(model_path, print_system_info=True)\n",
    "print(\"Model loaded âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ğŸ§± 2. OrtamÄ± HazÄ±rlayÄ±n\n",
    "\n",
    "EÄŸitilmiÅŸ modeli Ã§alÄ±ÅŸtÄ±rmadan Ã¶nce yeni bir bÃ¶lÃ¼m baÅŸlatmak iÃ§in ortamÄ± yÃ¼kleyin ve sÄ±fÄ±rlayÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env reset âœ… | obs shape/type: <class 'numpy.ndarray'> (1,)\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v1\", render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "obs = env.reset()  # VecEnv -> sadece obs dÃ¶ner (info yok)\n",
    "print(\"Env reset âœ… | obs shape/type:\", type(obs), getattr(obs, \"shape\", None))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. ğŸ§  En Ä°yi Sonraki Eylemi Tahmin Edin\n",
    "\n",
    "Mevcut gÃ¶zleme dayalÄ± olarak sonraki eylemi seÃ§mek iÃ§in modelin `.predict()` metodunu kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted action: [0]\n"
     ]
    }
   ],
   "source": [
    "action, state = model.predict(obs, deterministic=True)\n",
    "print(\"Predicted action:\", action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. ğŸš¶â€â™‚ï¸ AdÄ±mÄ± AtÄ±n\n",
    "\n",
    "- Tahmin edilen eylemi uygulamak iÃ§in `.step(action)` kullanÄ±n.  \n",
    "- Bu ÅŸunlarÄ± dÃ¶ndÃ¼rÃ¼r:\n",
    "  - Yeni durum  \n",
    "  - Ã–dÃ¼l  \n",
    "  - Bir `done` bayraÄŸÄ± (bÃ¶lÃ¼m bitmiÅŸ ise)  \n",
    "  - Ek bilgi (varsa)\n",
    "\n",
    "AjanÄ±nÄ±z garip hareketler yaparsa veya sÄ±kÄ±ÅŸÄ±rsa, paniklemeyÄ±n: muhtemelen yeterince uzun eÄŸitim gÃ¶rmemiÅŸtir. Daha uzun eÄŸitmeyi denemeden Ã¶nce meydan okumanÄ±n sonuna kadar gidin ğŸ˜‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New obs: [24]\n",
      "Reward: [-1.]\n",
      "Done: [False]\n",
      "Info: [{'prob': 1.0, 'TimeLimit.truncated': False}]\n"
     ]
    }
   ],
   "source": [
    "obs, rewards, dones, infos = env.step(action)\n",
    "\n",
    "print(\"New obs:\", obs)\n",
    "print(\"Reward:\", rewards)\n",
    "print(\"Done:\", dones)\n",
    "print(\"Info:\", infos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. ğŸ–¼ï¸ OrtamÄ± GÃ¶rselleÅŸtirin\n",
    "\n",
    "Eylem gerÃ§ekleÅŸtirildikten sonra ortamÄ±n mevcut durumunu gÃ¶rselleÅŸtirmek iÃ§in `.render()` kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rendered âœ…\n"
     ]
    }
   ],
   "source": [
    "env.render()\n",
    "print(\"Rendered âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.  ğŸ§¹ OrtamÄ± KapatÄ±n\n",
    "\n",
    "OrtamÄ± dÃ¼zgÃ¼n bir ÅŸekilde kapatmak ve sistem kaynaklarÄ±nÄ± serbest bÄ±rakmak iÃ§in `.close()` kullanÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env closed âœ…\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Env closed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### BÃ¶lÃ¼m 7: Tam EtkileÅŸim DÃ¶ngÃ¼sÃ¼nÃ¼ UygulayÄ±n\n",
    "\n",
    "Bu son alÄ±ÅŸtÄ±rmada, Ã¶nceden eÄŸitilmiÅŸ modelinizin bÃ¶lÃ¼m bitene kadar ortamla adÄ±m adÄ±m etkileÅŸim kurduÄŸu tam bir dÃ¶ngÃ¼ oluÅŸturacaksÄ±nÄ±z.\n",
    "\n",
    "- Eylemleri tekrarlamak iÃ§in bir `while` dÃ¶ngÃ¼sÃ¼ kullanÄ±n.\n",
    "- Eylemleri seÃ§mek iÃ§in `.predict()` ve bunlarÄ± uygulamak iÃ§in `.step()` Ã§aÄŸÄ±rÄ±n.\n",
    "- Bir sonraki predict iterasyonuna yeni durumu beslediÄŸinizden emin olun.\n",
    "- Ne zaman dÃ¶ngÃ¼den Ã§Ä±kacaÄŸÄ±nÄ±zÄ± bilmek iÃ§in `.step()`'den gelen `done` bayraÄŸÄ±nÄ± kullanÄ±n.\n",
    "- Her adÄ±mdan sonra ajanÄ± aksiyon halinde gÃ¶rselleÅŸtirmek iÃ§in `.render()` Ã§aÄŸÄ±rmayÄ± unutmayÄ±n.\n",
    "\n",
    "Yine, ajanÄ±nÄ±z garip hareketler seÃ§erse veya sonsuz dÃ¶ngÃ¼ye girerse, paniklemeyÄ±n: muhtemelen yeterince uzun eÄŸitim gÃ¶rmemiÅŸtir. AjanÄ±nÄ±z sonsuz dÃ¶ngÃ¼ye girdiyse, hÃ¼crenin yÃ¼rÃ¼tÃ¼lmesini durdurun.\n",
    "\n",
    "ğŸ‘‰ Daha uzun eÄŸitmeyi denemeden Ã¶nce meydan okumanÄ±n sonuna kadar gidin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": [
     "steps"
    ]
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "daed766e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: macOS-26.2-arm64-arm-64bit Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.7.1\n",
      "- PyTorch: 2.9.1\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.2\n",
      "- Gymnasium: 1.2.3\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: macOS-26.2-arm64-arm-64bit Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.7.1\n",
      "- PyTorch: 2.9.1\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.2\n",
      "- Gymnasium: 1.2.3\n",
      "\n",
      "Model loaded âœ…\n"
     ]
    }
   ],
   "source": [
    "model_path = \"dqn_cliffwalking_fast\"\n",
    "model = DQN.load(model_path, print_system_info=True)\n",
    "print(\"Model loaded âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "69613c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode started âœ… | initial obs: [36]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CliffWalking-v1\", render_mode=\"human\")\n",
    "env = DummyVecEnv([lambda: env])\n",
    "\n",
    "obs = env.reset()\n",
    "print(\"Episode started âœ… | initial obs:\", obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a448dc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step=001 | action=0 | reward=-1.0 | done=False\n",
      "step=002 | action=1 | reward=-1.0 | done=False\n",
      "step=003 | action=3 | reward=-1.0 | done=False\n",
      "step=004 | action=1 | reward=-1.0 | done=False\n",
      "step=005 | action=3 | reward=-1.0 | done=False\n",
      "step=006 | action=1 | reward=-1.0 | done=False\n",
      "step=007 | action=3 | reward=-1.0 | done=False\n",
      "step=008 | action=1 | reward=-1.0 | done=False\n",
      "step=009 | action=3 | reward=-1.0 | done=False\n",
      "step=010 | action=1 | reward=-1.0 | done=False\n",
      "step=011 | action=3 | reward=-1.0 | done=False\n",
      "step=012 | action=1 | reward=-1.0 | done=False\n",
      "step=013 | action=3 | reward=-1.0 | done=False\n",
      "step=014 | action=1 | reward=-1.0 | done=False\n",
      "step=015 | action=3 | reward=-1.0 | done=False\n",
      "step=016 | action=1 | reward=-1.0 | done=False\n",
      "step=017 | action=3 | reward=-1.0 | done=False\n",
      "step=018 | action=1 | reward=-1.0 | done=False\n",
      "step=019 | action=3 | reward=-1.0 | done=False\n",
      "step=020 | action=1 | reward=-1.0 | done=False\n",
      "step=021 | action=3 | reward=-1.0 | done=False\n",
      "step=022 | action=1 | reward=-1.0 | done=False\n",
      "step=023 | action=3 | reward=-1.0 | done=False\n",
      "step=024 | action=1 | reward=-1.0 | done=False\n",
      "step=025 | action=3 | reward=-1.0 | done=False\n",
      "step=026 | action=1 | reward=-1.0 | done=False\n",
      "step=027 | action=3 | reward=-1.0 | done=False\n",
      "step=028 | action=1 | reward=-1.0 | done=False\n",
      "step=029 | action=3 | reward=-1.0 | done=False\n",
      "step=030 | action=1 | reward=-1.0 | done=False\n",
      "step=031 | action=3 | reward=-1.0 | done=False\n",
      "step=032 | action=1 | reward=-1.0 | done=False\n",
      "step=033 | action=3 | reward=-1.0 | done=False\n",
      "step=034 | action=1 | reward=-1.0 | done=False\n",
      "step=035 | action=3 | reward=-1.0 | done=False\n",
      "step=036 | action=1 | reward=-1.0 | done=False\n",
      "step=037 | action=3 | reward=-1.0 | done=False\n",
      "step=038 | action=1 | reward=-1.0 | done=False\n",
      "step=039 | action=3 | reward=-1.0 | done=False\n",
      "step=040 | action=1 | reward=-1.0 | done=False\n",
      "step=041 | action=3 | reward=-1.0 | done=False\n",
      "step=042 | action=1 | reward=-1.0 | done=False\n",
      "step=043 | action=3 | reward=-1.0 | done=False\n",
      "step=044 | action=1 | reward=-1.0 | done=False\n",
      "step=045 | action=3 | reward=-1.0 | done=False\n",
      "step=046 | action=1 | reward=-1.0 | done=False\n",
      "step=047 | action=3 | reward=-1.0 | done=False\n",
      "step=048 | action=1 | reward=-1.0 | done=False\n",
      "step=049 | action=3 | reward=-1.0 | done=False\n",
      "step=050 | action=1 | reward=-1.0 | done=False\n",
      "step=051 | action=3 | reward=-1.0 | done=False\n",
      "step=052 | action=1 | reward=-1.0 | done=False\n",
      "step=053 | action=3 | reward=-1.0 | done=False\n",
      "step=054 | action=1 | reward=-1.0 | done=False\n",
      "step=055 | action=3 | reward=-1.0 | done=False\n",
      "step=056 | action=1 | reward=-1.0 | done=False\n",
      "step=057 | action=3 | reward=-1.0 | done=False\n",
      "step=058 | action=1 | reward=-1.0 | done=False\n",
      "step=059 | action=3 | reward=-1.0 | done=False\n",
      "step=060 | action=1 | reward=-1.0 | done=False\n",
      "step=061 | action=3 | reward=-1.0 | done=False\n",
      "step=062 | action=1 | reward=-1.0 | done=False\n",
      "step=063 | action=3 | reward=-1.0 | done=False\n",
      "step=064 | action=1 | reward=-1.0 | done=False\n",
      "step=065 | action=3 | reward=-1.0 | done=False\n",
      "step=066 | action=1 | reward=-1.0 | done=False\n",
      "step=067 | action=3 | reward=-1.0 | done=False\n",
      "step=068 | action=1 | reward=-1.0 | done=False\n",
      "step=069 | action=3 | reward=-1.0 | done=False\n",
      "step=070 | action=1 | reward=-1.0 | done=False\n",
      "step=071 | action=3 | reward=-1.0 | done=False\n",
      "step=072 | action=1 | reward=-1.0 | done=False\n",
      "step=073 | action=3 | reward=-1.0 | done=False\n",
      "step=074 | action=1 | reward=-1.0 | done=False\n",
      "step=075 | action=3 | reward=-1.0 | done=False\n",
      "step=076 | action=1 | reward=-1.0 | done=False\n",
      "step=077 | action=3 | reward=-1.0 | done=False\n",
      "step=078 | action=1 | reward=-1.0 | done=False\n",
      "step=079 | action=3 | reward=-1.0 | done=False\n",
      "step=080 | action=1 | reward=-1.0 | done=False\n",
      "step=081 | action=3 | reward=-1.0 | done=False\n",
      "step=082 | action=1 | reward=-1.0 | done=False\n",
      "step=083 | action=3 | reward=-1.0 | done=False\n",
      "step=084 | action=1 | reward=-1.0 | done=False\n",
      "step=085 | action=3 | reward=-1.0 | done=False\n",
      "step=086 | action=1 | reward=-1.0 | done=False\n",
      "step=087 | action=3 | reward=-1.0 | done=False\n",
      "step=088 | action=1 | reward=-1.0 | done=False\n",
      "step=089 | action=3 | reward=-1.0 | done=False\n",
      "step=090 | action=1 | reward=-1.0 | done=False\n",
      "step=091 | action=3 | reward=-1.0 | done=False\n",
      "step=092 | action=1 | reward=-1.0 | done=False\n",
      "step=093 | action=3 | reward=-1.0 | done=False\n",
      "step=094 | action=1 | reward=-1.0 | done=False\n",
      "step=095 | action=3 | reward=-1.0 | done=False\n",
      "step=096 | action=1 | reward=-1.0 | done=False\n",
      "step=097 | action=3 | reward=-1.0 | done=False\n",
      "step=098 | action=1 | reward=-1.0 | done=False\n",
      "step=099 | action=3 | reward=-1.0 | done=False\n",
      "step=100 | action=1 | reward=-1.0 | done=False\n",
      "step=101 | action=3 | reward=-1.0 | done=False\n",
      "step=102 | action=1 | reward=-1.0 | done=False\n",
      "step=103 | action=3 | reward=-1.0 | done=False\n",
      "step=104 | action=1 | reward=-1.0 | done=False\n",
      "step=105 | action=3 | reward=-1.0 | done=False\n",
      "step=106 | action=1 | reward=-1.0 | done=False\n",
      "step=107 | action=3 | reward=-1.0 | done=False\n",
      "step=108 | action=1 | reward=-1.0 | done=False\n",
      "step=109 | action=3 | reward=-1.0 | done=False\n",
      "step=110 | action=1 | reward=-1.0 | done=False\n",
      "step=111 | action=3 | reward=-1.0 | done=False\n",
      "step=112 | action=1 | reward=-1.0 | done=False\n",
      "step=113 | action=3 | reward=-1.0 | done=False\n",
      "step=114 | action=1 | reward=-1.0 | done=False\n",
      "step=115 | action=3 | reward=-1.0 | done=False\n",
      "step=116 | action=1 | reward=-1.0 | done=False\n",
      "step=117 | action=3 | reward=-1.0 | done=False\n",
      "step=118 | action=1 | reward=-1.0 | done=False\n",
      "step=119 | action=3 | reward=-1.0 | done=False\n",
      "step=120 | action=1 | reward=-1.0 | done=False\n",
      "step=121 | action=3 | reward=-1.0 | done=False\n",
      "step=122 | action=1 | reward=-1.0 | done=False\n",
      "step=123 | action=3 | reward=-1.0 | done=False\n",
      "step=124 | action=1 | reward=-1.0 | done=False\n",
      "step=125 | action=3 | reward=-1.0 | done=False\n",
      "step=126 | action=1 | reward=-1.0 | done=False\n",
      "step=127 | action=3 | reward=-1.0 | done=False\n",
      "step=128 | action=1 | reward=-1.0 | done=False\n",
      "step=129 | action=3 | reward=-1.0 | done=False\n",
      "step=130 | action=1 | reward=-1.0 | done=False\n",
      "step=131 | action=3 | reward=-1.0 | done=False\n",
      "step=132 | action=1 | reward=-1.0 | done=False\n",
      "step=133 | action=3 | reward=-1.0 | done=False\n",
      "step=134 | action=1 | reward=-1.0 | done=False\n",
      "step=135 | action=3 | reward=-1.0 | done=False\n",
      "step=136 | action=1 | reward=-1.0 | done=False\n",
      "step=137 | action=3 | reward=-1.0 | done=False\n",
      "step=138 | action=1 | reward=-1.0 | done=False\n",
      "step=139 | action=3 | reward=-1.0 | done=False\n",
      "step=140 | action=1 | reward=-1.0 | done=False\n",
      "step=141 | action=3 | reward=-1.0 | done=False\n",
      "step=142 | action=1 | reward=-1.0 | done=False\n",
      "step=143 | action=3 | reward=-1.0 | done=False\n",
      "step=144 | action=1 | reward=-1.0 | done=False\n",
      "step=145 | action=3 | reward=-1.0 | done=False\n",
      "step=146 | action=1 | reward=-1.0 | done=False\n",
      "step=147 | action=3 | reward=-1.0 | done=False\n",
      "step=148 | action=1 | reward=-1.0 | done=False\n",
      "step=149 | action=3 | reward=-1.0 | done=False\n",
      "step=150 | action=1 | reward=-1.0 | done=False\n",
      "step=151 | action=3 | reward=-1.0 | done=False\n",
      "step=152 | action=1 | reward=-1.0 | done=False\n",
      "step=153 | action=3 | reward=-1.0 | done=False\n",
      "step=154 | action=1 | reward=-1.0 | done=False\n",
      "step=155 | action=3 | reward=-1.0 | done=False\n",
      "step=156 | action=1 | reward=-1.0 | done=False\n",
      "step=157 | action=3 | reward=-1.0 | done=False\n",
      "step=158 | action=1 | reward=-1.0 | done=False\n",
      "step=159 | action=3 | reward=-1.0 | done=False\n",
      "step=160 | action=1 | reward=-1.0 | done=False\n",
      "step=161 | action=3 | reward=-1.0 | done=False\n",
      "step=162 | action=1 | reward=-1.0 | done=False\n",
      "step=163 | action=3 | reward=-1.0 | done=False\n",
      "step=164 | action=1 | reward=-1.0 | done=False\n",
      "step=165 | action=3 | reward=-1.0 | done=False\n",
      "step=166 | action=1 | reward=-1.0 | done=False\n",
      "step=167 | action=3 | reward=-1.0 | done=False\n",
      "step=168 | action=1 | reward=-1.0 | done=False\n",
      "step=169 | action=3 | reward=-1.0 | done=False\n",
      "step=170 | action=1 | reward=-1.0 | done=False\n",
      "step=171 | action=3 | reward=-1.0 | done=False\n",
      "step=172 | action=1 | reward=-1.0 | done=False\n",
      "step=173 | action=3 | reward=-1.0 | done=False\n",
      "step=174 | action=1 | reward=-1.0 | done=False\n",
      "step=175 | action=3 | reward=-1.0 | done=False\n",
      "step=176 | action=1 | reward=-1.0 | done=False\n",
      "step=177 | action=3 | reward=-1.0 | done=False\n",
      "step=178 | action=1 | reward=-1.0 | done=False\n",
      "step=179 | action=3 | reward=-1.0 | done=False\n",
      "step=180 | action=1 | reward=-1.0 | done=False\n",
      "step=181 | action=3 | reward=-1.0 | done=False\n",
      "step=182 | action=1 | reward=-1.0 | done=False\n",
      "step=183 | action=3 | reward=-1.0 | done=False\n",
      "step=184 | action=1 | reward=-1.0 | done=False\n",
      "step=185 | action=3 | reward=-1.0 | done=False\n",
      "step=186 | action=1 | reward=-1.0 | done=False\n",
      "step=187 | action=3 | reward=-1.0 | done=False\n",
      "step=188 | action=1 | reward=-1.0 | done=False\n",
      "step=189 | action=3 | reward=-1.0 | done=False\n",
      "step=190 | action=1 | reward=-1.0 | done=False\n",
      "step=191 | action=3 | reward=-1.0 | done=False\n",
      "step=192 | action=1 | reward=-1.0 | done=False\n",
      "step=193 | action=3 | reward=-1.0 | done=False\n",
      "step=194 | action=1 | reward=-1.0 | done=False\n",
      "step=195 | action=3 | reward=-1.0 | done=False\n",
      "step=196 | action=1 | reward=-1.0 | done=False\n",
      "step=197 | action=3 | reward=-1.0 | done=False\n",
      "step=198 | action=1 | reward=-1.0 | done=False\n",
      "step=199 | action=3 | reward=-1.0 | done=False\n",
      "step=200 | action=1 | reward=-1.0 | done=False\n",
      "step=201 | action=3 | reward=-1.0 | done=False\n",
      "step=202 | action=1 | reward=-1.0 | done=False\n",
      "step=203 | action=3 | reward=-1.0 | done=False\n",
      "step=204 | action=1 | reward=-1.0 | done=False\n",
      "step=205 | action=3 | reward=-1.0 | done=False\n",
      "step=206 | action=1 | reward=-1.0 | done=False\n",
      "step=207 | action=3 | reward=-1.0 | done=False\n",
      "step=208 | action=1 | reward=-1.0 | done=False\n",
      "step=209 | action=3 | reward=-1.0 | done=False\n",
      "step=210 | action=1 | reward=-1.0 | done=False\n",
      "step=211 | action=3 | reward=-1.0 | done=False\n",
      "step=212 | action=1 | reward=-1.0 | done=False\n",
      "step=213 | action=3 | reward=-1.0 | done=False\n",
      "step=214 | action=1 | reward=-1.0 | done=False\n",
      "step=215 | action=3 | reward=-1.0 | done=False\n",
      "step=216 | action=1 | reward=-1.0 | done=False\n",
      "step=217 | action=3 | reward=-1.0 | done=False\n",
      "step=218 | action=1 | reward=-1.0 | done=False\n",
      "step=219 | action=3 | reward=-1.0 | done=False\n",
      "step=220 | action=1 | reward=-1.0 | done=False\n",
      "step=221 | action=3 | reward=-1.0 | done=False\n",
      "step=222 | action=1 | reward=-1.0 | done=False\n",
      "step=223 | action=3 | reward=-1.0 | done=False\n",
      "step=224 | action=1 | reward=-1.0 | done=False\n",
      "step=225 | action=3 | reward=-1.0 | done=False\n",
      "step=226 | action=1 | reward=-1.0 | done=False\n",
      "step=227 | action=3 | reward=-1.0 | done=False\n",
      "step=228 | action=1 | reward=-1.0 | done=False\n",
      "step=229 | action=3 | reward=-1.0 | done=False\n",
      "step=230 | action=1 | reward=-1.0 | done=False\n",
      "step=231 | action=3 | reward=-1.0 | done=False\n",
      "step=232 | action=1 | reward=-1.0 | done=False\n",
      "step=233 | action=3 | reward=-1.0 | done=False\n",
      "step=234 | action=1 | reward=-1.0 | done=False\n",
      "step=235 | action=3 | reward=-1.0 | done=False\n",
      "step=236 | action=1 | reward=-1.0 | done=False\n",
      "step=237 | action=3 | reward=-1.0 | done=False\n",
      "step=238 | action=1 | reward=-1.0 | done=False\n",
      "step=239 | action=3 | reward=-1.0 | done=False\n",
      "step=240 | action=1 | reward=-1.0 | done=False\n",
      "step=241 | action=3 | reward=-1.0 | done=False\n",
      "step=242 | action=1 | reward=-1.0 | done=False\n",
      "step=243 | action=3 | reward=-1.0 | done=False\n",
      "step=244 | action=1 | reward=-1.0 | done=False\n",
      "step=245 | action=3 | reward=-1.0 | done=False\n",
      "step=246 | action=1 | reward=-1.0 | done=False\n",
      "step=247 | action=3 | reward=-1.0 | done=False\n",
      "step=248 | action=1 | reward=-1.0 | done=False\n",
      "step=249 | action=3 | reward=-1.0 | done=False\n",
      "step=250 | action=1 | reward=-1.0 | done=False\n",
      "step=251 | action=3 | reward=-1.0 | done=False\n",
      "step=252 | action=1 | reward=-1.0 | done=False\n",
      "step=253 | action=3 | reward=-1.0 | done=False\n",
      "step=254 | action=1 | reward=-1.0 | done=False\n",
      "step=255 | action=3 | reward=-1.0 | done=False\n",
      "step=256 | action=1 | reward=-1.0 | done=False\n",
      "step=257 | action=3 | reward=-1.0 | done=False\n",
      "step=258 | action=1 | reward=-1.0 | done=False\n",
      "step=259 | action=3 | reward=-1.0 | done=False\n",
      "step=260 | action=1 | reward=-1.0 | done=False\n",
      "step=261 | action=3 | reward=-1.0 | done=False\n",
      "step=262 | action=1 | reward=-1.0 | done=False\n",
      "step=263 | action=3 | reward=-1.0 | done=False\n",
      "step=264 | action=1 | reward=-1.0 | done=False\n",
      "step=265 | action=3 | reward=-1.0 | done=False\n",
      "step=266 | action=1 | reward=-1.0 | done=False\n",
      "step=267 | action=3 | reward=-1.0 | done=False\n",
      "step=268 | action=1 | reward=-1.0 | done=False\n",
      "step=269 | action=3 | reward=-1.0 | done=False\n",
      "step=270 | action=1 | reward=-1.0 | done=False\n",
      "step=271 | action=3 | reward=-1.0 | done=False\n",
      "step=272 | action=1 | reward=-1.0 | done=False\n",
      "step=273 | action=3 | reward=-1.0 | done=False\n",
      "step=274 | action=1 | reward=-1.0 | done=False\n",
      "step=275 | action=3 | reward=-1.0 | done=False\n",
      "step=276 | action=1 | reward=-1.0 | done=False\n",
      "step=277 | action=3 | reward=-1.0 | done=False\n",
      "step=278 | action=1 | reward=-1.0 | done=False\n",
      "step=279 | action=3 | reward=-1.0 | done=False\n",
      "step=280 | action=1 | reward=-1.0 | done=False\n",
      "step=281 | action=3 | reward=-1.0 | done=False\n",
      "step=282 | action=1 | reward=-1.0 | done=False\n",
      "step=283 | action=3 | reward=-1.0 | done=False\n",
      "step=284 | action=1 | reward=-1.0 | done=False\n",
      "step=285 | action=3 | reward=-1.0 | done=False\n",
      "step=286 | action=1 | reward=-1.0 | done=False\n",
      "step=287 | action=3 | reward=-1.0 | done=False\n",
      "step=288 | action=1 | reward=-1.0 | done=False\n",
      "step=289 | action=3 | reward=-1.0 | done=False\n",
      "step=290 | action=1 | reward=-1.0 | done=False\n",
      "step=291 | action=3 | reward=-1.0 | done=False\n",
      "step=292 | action=1 | reward=-1.0 | done=False\n",
      "step=293 | action=3 | reward=-1.0 | done=False\n",
      "step=294 | action=1 | reward=-1.0 | done=False\n",
      "step=295 | action=3 | reward=-1.0 | done=False\n",
      "step=296 | action=1 | reward=-1.0 | done=False\n",
      "step=297 | action=3 | reward=-1.0 | done=False\n",
      "step=298 | action=1 | reward=-1.0 | done=False\n",
      "step=299 | action=3 | reward=-1.0 | done=False\n",
      "step=300 | action=1 | reward=-1.0 | done=False\n",
      "\n",
      "Episode finished âœ…\n",
      "Total steps: 300\n",
      "Total reward: -300.0\n",
      "Last info: {'prob': 1.0, 'TimeLimit.truncated': False}\n"
     ]
    }
   ],
   "source": [
    "max_steps = 300   # gÃ¼venlik bariyeri\n",
    "step_count = 0\n",
    "total_reward = 0.0\n",
    "\n",
    "done = False\n",
    "\n",
    "while not done and step_count < max_steps:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "\n",
    "    obs, rewards, dones, infos = env.step(action)\n",
    "\n",
    "    # VecEnv olduÄŸu iÃ§in rewards/dones array; tek env var -> 0. eleman\n",
    "    r = float(rewards[0])\n",
    "    d = bool(dones[0])\n",
    "\n",
    "    total_reward += r\n",
    "    step_count += 1\n",
    "    done = d\n",
    "\n",
    "    env.render()\n",
    "\n",
    "    # Ä°stersen adÄ±mlarÄ± ekrana dÃ¶k (Ã§ok spam olmasÄ±n diye kÄ±sa tuttum)\n",
    "    print(f\"step={step_count:03d} | action={int(action[0])} | reward={r} | done={done}\")\n",
    "\n",
    "    # animasyon daha anlaÅŸÄ±lÄ±r olsun diye (istersen kapat)\n",
    "    time.sleep(0.05)\n",
    "\n",
    "print(\"\\nEpisode finished âœ…\")\n",
    "print(\"Total steps:\", step_count)\n",
    "print(\"Total reward:\", total_reward)\n",
    "print(\"Last info:\", infos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3daf211c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Env closed âœ…\n"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "print(\"Env closed âœ…\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” AjanÄ±nÄ±zÄ±n DavranÄ±ÅŸÄ±ndan Memnun DeÄŸil misiniz?\n",
    "\n",
    "EndiÅŸelenmeyin â€” seÃ§enekleriniz var:\n",
    "\n",
    "- ğŸ‹ï¸â€â™‚ï¸ **Daha uzun eÄŸitin** â†’ PerformansÄ± artÄ±rmak iÃ§in bÃ¶lÃ¼m sayÄ±sÄ±nÄ± artÄ±rmayÄ± deneyin.\n",
    "- ğŸ“¦ **Veya bizim Ã¶nceden eÄŸitilmiÅŸ modelimizi yÃ¼kleyin** â†’  \n",
    "  **500.000 bÃ¶lÃ¼m** iÃ§in bir tane eÄŸittik â€” indirmek iÃ§in aÅŸaÄŸÄ±daki hÃ¼crenin yorumunu kaldÄ±rÄ±n ve Ã§alÄ±ÅŸtÄ±rÄ±n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100  139k  100  139k    0     0   168k      0 --:--:-- --:--:-- --:--:--  168k\n"
     ]
    }
   ],
   "source": [
    "!curl https://d37p7d5kaxknzw.cloudfront.net/projects/best_dqn_cliffwalking.zip > best_dqn_cliffwalking.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e05a05b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: macOS-26.2-arm64-arm-64bit Darwin Kernel Version 25.2.0: Tue Nov 18 21:09:56 PST 2025; root:xnu-12377.61.12~1/RELEASE_ARM64_T6041\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.7.1\n",
      "- PyTorch: 2.9.1\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.2\n",
      "- Gymnasium: 1.2.3\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: macOS-14.1-x86_64-i386-64bit Darwin Kernel Version 23.1.0: Mon Oct  9 21:27:27 PDT 2023; root:xnu-10002.41.9~6/RELEASE_X86_64\n",
      "- Python: 3.12.9\n",
      "- Stable-Baselines3: 2.4.1\n",
      "- PyTorch: 2.2.2\n",
      "- GPU Enabled: False\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 3.1.1\n",
      "- Gymnasium: 1.0.0\n",
      "\n",
      "Best model loaded âœ…\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "best_model = DQN.load(\"best_dqn_cliffwalking.zip\", print_system_info=True)\n",
    "print(\"Best model loaded âœ…\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "48a1a701",
   "metadata": {},
   "outputs": [],
   "source": [
    "action, _ = best_model.predict(obs, deterministic=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### â„ï¸ Son not: `is_slippery=True` Anlamak\n",
    "\n",
    "DokÃ¼mantasyonu okurken `is_slippery` parametresinin ne yaptÄ±ÄŸÄ±nÄ± merak etmiÅŸ olabilirsiniz.\n",
    "\n",
    "`CliffWalking`'de `is_slippery=True` ayarlamak, ajanÄ±n eylemlerine rastgelelik ekler â€” ajanÄ±n kayÄ±p istenmeyen bir yÃ¶ne hareket etme ÅŸansÄ±nÄ± tanÄ±tÄ±r. `is_slippery=True` ile eÄŸitim yapmak, belirsizlikle baÅŸa Ã§Ä±kabilen ajanlar oluÅŸturur â€” gerÃ§ek dÃ¼nya uygulamalarÄ± iÃ§in kritik bir beceri.\n",
    "\n",
    "#### ğŸ¯ Neden stokastisite ekleyelim?\n",
    "\n",
    "- **ğŸŒ GerÃ§ekÃ§ilik** â†’ GerÃ§ek dÃ¼nya ortamlarÄ±nda bulunan belirsizliÄŸi simÃ¼le eder.  \n",
    "- **ğŸ›¡ï¸ SaÄŸlamlÄ±k** â†’ AjanlarÄ±n daha gÃ¼venilir, uyum saÄŸlayabilen stratejiler Ã¶ÄŸrenmesine yardÄ±mcÄ± olur.  \n",
    "- **ğŸ”¥ Meydan okuma** â†’ GÃ¶revi daha zor ve Ã§Ã¶zÃ¼lmesi daha ilginÃ§ hale getirir.\n",
    "\n",
    "#### ğŸ¤” Peki neden burada `True` yapmadÄ±k?\n",
    "\n",
    "DQN algoritmasÄ± basit kullanÄ±m durumlarÄ± iÃ§in tasarlanmÄ±ÅŸtÄ±r. `slippery=True`'nun `CliffWalking` ortamÄ±na eklediÄŸi yÃ¼ksek stokastisite ile mÃ¼cadele eder. Åunu dÃ¼ÅŸÃ¼nÃ¼n: bu basit ortamda, amaÃ§lanan yÃ¶nÃ¼ takip etmemek hemen tamamen farklÄ± bir yÃ¶ne gitmek anlamÄ±na gelir: 90 derece, hatta 180 derece! Basit bir algoritmanÄ±n Ã¶ÄŸrenmesi Ã§ok zor olurdu."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Projeyi tamamladÄ±ÄŸÄ±nÄ±z iÃ§in tebrikler ğŸ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workintech_current",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
